Name: Katherine Li
NetID: kl261
Hours Spent: Started on February 12, 2017; finished on February 16, 2017; ~10 hours
Consulted With: Peyton Schafer
Resources Used: None (other than Java API)
Impressions: While this assignment was extremely interesting and definitely helped me better understand
HashMaps, TreeMaps, Arrays, ArrayLists, etc., the part I disliked the most was oftentimes how confusing
and vague the instructions were. I understand them being vague to not give away the solution but there
were too many times where the only reason I couldn't do a part of the assignment was because the instructions/
questions made it too confusing for me to decide on a course of action.
----------------------------------------------------------------------
PART A: TIMING
PROBLEM A.1: 
alice.txt, k = 1, 200 characters max: 0.210
alice.txt, k = 1, 400 characters max: 0.311
alice.txt, k = 1, 800 characters max: 0.540
alice.txt, k = 5, 200 characters max: 0.071
alice.txt, k = 5, 400 characters max: 0.133
alice.txt, k = 5, 800 characters max: 0.233
alice.txt, k = 10, 200 characters max: 0.069
alice.txt, k = 10, 400 characters max: 0.107
alice.txt, k = 10, 800 characters max: 0.206
hawthorne.txt, k = 1, 200 characters max: 0.432
hawthorne.txt, k = 1, 400 characters max: 0.782
hawthorne.txt, k = 1, 800 characters max: 1.666

PROBLEM A.2: 2.Based on these results, what is the relationship between the
runtime and the length of the training text, the k-value, and the max
characters generated? How long do you think it will take to generate 1600
random characters using an order-5 Markov model when the The Complete Works
of William Shakespeare is used as the training text — our online copy of
this text contains roughly 5.5 million characters. Justify your answer —
don’t test empirically, use reasoning.

The relationship between runtime and the length of the training text and the max characters 
generated is that the longer the text and the larger the max characters, the longer the runtime. 
On the other hand, the larger the k-value the smaller the runtime. To generate 1600 random 
characters with an order-5 Markov model from the Shapespeare text, it would most likely take 
around 6 seconds because BruteMarkov would have to scan through all 5.5 million characters every 
time it generates 'follows' for each Markov n-gram and it can generate up to 1600 random characters 
in n-grams of size 5. The order size is not high enough to decrease the amount of time it would 
take to scan through the entire text to generate that many characters. Assuming that Shakespeare 
is ~5 times longer than hawthorne.txt, as number of max characters double the runtime roughly 
doubles, and the order-5 Markov decreases the time count by about half (compared to the order-1 
Markov), it would most likely take anywhere between 5 and 8 seconds to run.

PROBLEM A.3: Provide timings using your Map/Smart model for both creating
the map and generating 200, 400, 800, and 1600 character random texts with
an order-5 Model and alice.txt. Provide some explanation for the timings
you observe.

alice.txt, k = 5, 200 characters max: 0.130
alice.txt, k = 5, 400 characters max: 0.108
alice.txt, k = 5, 800 characters max: 0.124
alice.txt, k = 5, 1600 characters max: 0.125

Using a map/smart model (EfficientMarkov) generates the approximate runtimes above that are
all close to the same value (a value that is significantly lower than the runtimes in BruteMarkov -
not the initial 200 char BruteMarkov but definitely the larger char values) despite the increasing 
character max because EfficientMarkov only has to scan through the original text file once. From 
the first scan it creates a map that maps n-grams to the following characters. The process of making 
a map might take longer than just generating 200 characters using BruteMarkov but it definitely makes 
a difference as max character values go up. That map is then accessed whenever the code calls for 
'follows' so it does not matter how many characters the code asks for since getting values based on 
keys from a map is extremely fast (i.e. more characters would not cause a significant delay in the 
program as in BruteMarkov).

PROBLEM A.4: Provide timings for the EfficientWordMarkov with different
hashcode methods. Time the method you are given and compare the results
that you achieve with the better hashcode method that you developed.

alice.txt, k = 5, 1600 characters max, basic hashcode: 0.111
alice.txt, k = 5, 1600 characters max, better hashcode: 0.051

The runtime is significantly shorter for the better hashcode method because it minimizes the 
number of hash collisions (since basic hashcode was just making all of the hashcodes equal to 32), 
which makes for a more efficient HashMap and thus more efficient program/code.

PROBLEM A.5: Using a k of your choice and clinton-convention.txt as
the training text, if we do not set a maximum number of characters
generated (you can effectively do this by having maxLetters be a large
number, e.g. 1000 times the size of the training text) what is the
average number of characters generated by our Markov text generator?

Using a k of 5, the average number of characters generated by EfficientMarkov is 5462.

----------------------------------------------------------------------
PART B: HASHMAP VS. TREEMAP FOR EfficientWordMarkov
What are the differences in performance between HashMap and TreeMap as the
number of NGrams in the map grow?

Performance of EfficientWordMarkov using HashMaps compared to TreeMaps was assessed by graphing number 
of keys produced against runtime required to generate 1600 words using an order-5 Markov model. Although 
there wasn't a huge difference between the runtimes of EfficientWordMarkov using HashMaps and using TreeMaps 
when the number of keys was low (e.g. for trump-sept15.txt number of keys as 3051 and runtime for HashMaps 
was 0.041 s while runtime for TreeMaps was 0.046 s), there was a significant difference in runtimes when 
number of keys was larger (e.g. for kjv10.txt number of keys was 753994 and runtime for HashMaps was 1.657 s 
and runtime for TreeMaps was 6.458 s). This trend is supported by the shape of the graph, which shows both 
sets of data values increasing linearly (i.e. as number of keys increased the runtime increased) and as number
of keys gets larger, the difference in runtime between HashMaps and TreeMaps gets larger. This is most likely
because HashMaps are unordered while TreeMaps are ordered, so although they both run similarly quickly when there
are few keys to map to values, HashMaps become significantly faster when dealing with more keys to map to values
because the HashMap doesn't have to worry about also sorting the keys like TreeMaps do.

Note: images of the graph and the data used to generate the graph are included in the git repository


